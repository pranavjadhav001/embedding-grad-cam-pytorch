{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d2092b6-d7fd-4e13-8c13-737b2ad599f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import copy\n",
    "import os.path as osp\n",
    "import click\n",
    "import cv2\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms,datasets\n",
    "import PIL\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import glob\n",
    "from grad_cam import (\n",
    "    BackPropagation,\n",
    "    Deconvnet,\n",
    "    GradCAM,\n",
    "    GuidedBackPropagation,\n",
    ")\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1827916e-1c9e-4db7-be8f-e1f2e13cc24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class for loading image and label\n",
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, image_paths,labels,transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.load_image_from_paths()\n",
    "        \n",
    "    def load_image_from_paths(self):\n",
    "        self.images = []\n",
    "        for i in self.image_paths:\n",
    "            img = PIL.Image.open(i)\n",
    "            if len(img.getbands()) ==1 :\n",
    "                img = img.convert(\"RGB\")\n",
    "            self.images.append(img.resize((224,224)))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c160f228-b298-4a91-a008-8d5ce9217257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(cuda):\n",
    "    cuda = cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "    if cuda:\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(\"Device:\", torch.cuda.get_device_name(current_device))\n",
    "    else:\n",
    "        print(\"Device: CPU\")\n",
    "    return device\n",
    "\n",
    "def load_images(image_paths):\n",
    "    images = []\n",
    "    raw_images = []\n",
    "    print(\"Images:\")\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(\"\\t#{}: {}\".format(i, image_path))\n",
    "        image, raw_image = preprocess(image_path)\n",
    "        images.append(image)\n",
    "        raw_images.append(raw_image)\n",
    "    return images, raw_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e0c8f23-adca-499c-97c0-3d507c490e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean, std)])\n",
    "def preprocess(image_path):\n",
    "    raw_image = PIL.Image.open(image_path).resize((224,224))\n",
    "    if len(raw_image.getbands()) ==1 :\n",
    "        raw_image = raw_image.convert(\"RGB\")       \n",
    "    image = test_transform(raw_image.copy())\n",
    "    return image, raw_image\n",
    "\n",
    "def save_gradient(filename, gradient):\n",
    "    gradient = gradient.cpu().numpy().transpose(1, 2, 0)\n",
    "    gradient -= gradient.min()\n",
    "    gradient /= gradient.max()\n",
    "    gradient *= 255.0\n",
    "    cv2.imwrite(filename, np.uint8(gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e44ded0-1acc-4f0c-a8a0-1cf6403e9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gradcam(filename, gcam, raw_image, paper_cmap=False):\n",
    "    gcam = gcam.cpu().numpy()\n",
    "    cmap = cm.jet_r(gcam)[..., :3] * 255.0\n",
    "    if paper_cmap:\n",
    "        alpha = gcam[..., None]\n",
    "        gcam = alpha * cmap + (1 - alpha) * raw_image\n",
    "    else:\n",
    "        gcam = (cmap.astype(np.float64) + raw_image.astype(np.float64)) / 2\n",
    "    cv2.imwrite(filename, np.uint8(gcam))\n",
    "\n",
    "def save_sensitivity(filename, maps):\n",
    "    maps = maps.cpu().numpy()\n",
    "    scale = max(maps[maps > 0].max(), -maps[maps <= 0].min())\n",
    "    maps = maps / scale * 0.5\n",
    "    maps += 0.5\n",
    "    maps = cm.bwr_r(maps)[..., :3]\n",
    "    maps = np.uint8(maps * 255.0)\n",
    "    maps = cv2.resize(maps, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "    cv2.imwrite(filename, maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e9e310-9429-422e-a567-19f4ae9081bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir embedding_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14da51d2-4e5b-4bb8-b488-8aaf6b610ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "807036e9-c2f2-40d8-8163-5d6cdbe5d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(models.resnet18())\n",
    "class ResNetFeatrueExtractor50(nn.Module):\n",
    "    def __init__(self, pretrained = True):\n",
    "        super(ResNetFeatrueExtractor50, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=pretrained)\n",
    "        self.model.fc = nn.Linear(2048, config['embedding_dim'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f077438-53b4-4998-b914-e369b60e0ba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetFeatrueExtractor50(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "output_dir = 'embedding_results'\n",
    "topk = 1\n",
    "#load model weights finetuned using EPSHN triplet loss on CUB_200_2011 Dataset with all 200 classes\n",
    "model = torch.load('../models/cub_triplet_loss_epshn_resnet50_sgd_aug_200.pth',map_location='cuda')\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad3337d0-ec63-4ba6-a7e0-ecaa8342695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load class name dict\n",
    "with open('../CUB_200_2011/classes.txt','r') as f:\n",
    "    classes = f.readlines()\n",
    "classes = [i.replace('\\n','') for i in classes]\n",
    "classes = [i.split(' ')[1] for i in classes]\n",
    "class_dict = {k:v for k,v in zip(classes,range(200))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c689fca-3497-40cb-8225-ff39b6bd9ba3",
   "metadata": {},
   "source": [
    "## Create Proxy Embeddings for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414e8c93-3211-4214-b918-d29053826476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proxy_class = dict() #store class proxies against class names\n",
    "for folder_path,i in class_dict.items():#iterate over class names\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    folder_images = glob.glob('../CUB_200_2011/images/'+'/'+str(folder_path)+'/*')\n",
    "    image_paths.extend(folder_images)\n",
    "    labels.extend([i]*len(folder_images))\n",
    "    all_class_embeddings = []\n",
    "    train_dataset  = CUBDataset(image_paths,labels,test_transform)# load dataset for this class\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)#create dataloader for this class\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        embeddings = model(data)#get embeddings \n",
    "        all_class_embeddings.extend(embeddings.detach().cpu().numpy())\n",
    "    \n",
    "    all_class_embeddings = torch.as_tensor(np.asarray(all_class_embeddings))\n",
    "    class_weight = torch.nn.functional.normalize(torch.unsqueeze(torch.mean(all_class_embeddings,axis=0),dim=0),\n",
    "                                                 p=2.0, dim=-1).to(device)#equation 11 & 12\n",
    "    proxy_class[str(folder_path)] = {'proxy':class_weight[0],'image_paths':image_paths,'labels':labels}#save class proxy\n",
    "    #break#consider only 1 class for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5175b7fc-2cf7-44ae-8186-c4cbdab31e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = '188.Pileated_Woodpecker'\n",
    "proxy_embedding = proxy_class[class_name]['proxy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0998e5a-fb05-4e7c-803d-90e9e3d336be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images:\n",
      "\t#0: ../CUB_200_2011/images/188.Pileated_Woodpecker/Pileated_Woodpecker_0034_180419.jpg\n"
     ]
    }
   ],
   "source": [
    "# Images\n",
    "images, raw_images = load_images([random.choice(\n",
    "    glob.glob(os.path.join('../CUB_200_2011/images/',class_name+'/*')))])\n",
    "images = torch.stack(images).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54e397b0-2dd3-4f1a-a3b9-5ff4741b7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images:\n",
      "\t#0: ../CUB_200_2011/images/188.Pileated_Woodpecker/Pileated_Woodpecker_0034_180419.jpg\n"
     ]
    }
   ],
   "source": [
    "image_path = '../CUB_200_2011/images/188.Pileated_Woodpecker/Pileated_Woodpecker_0034_180419.jpg'\n",
    "images, raw_images = load_images([image_path])\n",
    "images = torch.stack(images).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0bda946-6334-4bfc-a396-bca00d71d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [\"model.layer1\", \"model.layer2\", \"model.layer3\", \"model.layer4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7879d42-408c-4d44-9ec3-7aba5ffa58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = target_layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643eefb-b0bb-4776-998a-6c4c49d0f2ec",
   "metadata": {},
   "source": [
    "## Vanilla BackPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "211c16df-f747-44da-9b1d-db3bdbd7f2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Backpropagation:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Common usage:\n",
    "1. Wrap your model with visualization classes defined in grad_cam.py\n",
    "2. Run forward() with images\n",
    "3. Run backward() with a list of specific classes\n",
    "4. Run generate() to export results\n",
    "\"\"\"\n",
    "\n",
    "# =========================================================================\n",
    "print(\"Vanilla Backpropagation:\")\n",
    "\n",
    "bp = BackPropagation(model=model,proxy_embeddings=proxy_embedding)\n",
    "_= bp.forward(images)  # sorted\n",
    "\n",
    "for i in range(topk):\n",
    "    bp.backward()\n",
    "    gradients = bp.generate()\n",
    "\n",
    "    # Save results as image files\n",
    "    for j in range(len(images)):\n",
    "        #print(\"\\t#{}: {} ({:.5f})\".format(j, classes[ids[j, i]], probs[j, i]))\n",
    "\n",
    "        save_gradient(\n",
    "            filename=osp.join(\n",
    "                output_dir,\n",
    "                \"{}-vanilla_200.png\".format(j),\n",
    "            ),\n",
    "            gradient=gradients[j],\n",
    "        )\n",
    "\n",
    "# Remove all the hook function in the \"model\"\n",
    "bp.remove_hook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f29aa-da82-4401-bab4-5d4f4c3abfc4",
   "metadata": {},
   "source": [
    "## Deconvnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b43b46-ca9f-4509-a6f1-01be70287530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deconvolution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "print(\"Deconvolution:\")\n",
    "\n",
    "deconv = Deconvnet(model=model,proxy_embeddings=proxy_embedding)\n",
    "_ = deconv.forward(images)\n",
    "\n",
    "for i in range(topk):\n",
    "    deconv.backward()\n",
    "    gradients = deconv.generate()\n",
    "\n",
    "    for j in range(len(images)):\n",
    "\n",
    "        save_gradient(\n",
    "            filename=osp.join(\n",
    "                output_dir,\n",
    "                \"{}-deconvnet_200.png\".format(j),\n",
    "            ),\n",
    "            gradient=gradients[j],\n",
    "        )\n",
    "\n",
    "deconv.remove_hook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6825a7de-b2cc-486c-9c69-aab683523583",
   "metadata": {},
   "source": [
    "## Grad-CAM | Guided Backpropagation | Guided Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f2c189b-ccf8-47a4-b88e-4107f19dc3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad-CAM/Guided Backpropagation/Guided Grad-CAM:\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "print(\"Grad-CAM/Guided Backpropagation/Guided Grad-CAM:\")\n",
    "\n",
    "gcam = GradCAM(model=model,proxy_embeddings=proxy_embedding)\n",
    "_ = gcam.forward(images)\n",
    "\n",
    "gbp = GuidedBackPropagation(model=model,proxy_embeddings=proxy_embedding)\n",
    "_ = gbp.forward(images)\n",
    "\n",
    "for i in range(topk):\n",
    "    # Guided Backpropagation\n",
    "    gbp.backward()\n",
    "    gradients = gbp.generate()\n",
    "\n",
    "    # Grad-CAM\n",
    "    gcam.backward()\n",
    "    regions = gcam.generate(target_layer=target_layer)\n",
    "\n",
    "    for j in range(len(images)):\n",
    "\n",
    "        # Guided Backpropagation\n",
    "        save_gradient(\n",
    "            filename=osp.join(\n",
    "                output_dir,\n",
    "                \"{}-guided_200.png\".format(j),\n",
    "            ),\n",
    "            gradient=gradients[j],\n",
    "        )\n",
    "\n",
    "        # Grad-CAM\n",
    "        save_gradcam(\n",
    "            filename=osp.join(\n",
    "                output_dir,\n",
    "                \"{}-gradcam-{}_200.png\".format(\n",
    "                    j,target_layer\n",
    "                ),\n",
    "            ),\n",
    "            gcam=regions[j, 0],\n",
    "            raw_image=raw_images[j],\n",
    "            paper_cmap=True\n",
    "        )\n",
    "\n",
    "        # Guided Grad-CAM\n",
    "        save_gradient(\n",
    "            filename=osp.join(\n",
    "                output_dir,\n",
    "                \"{}-guided-gradcam-{}_200.png\".format(\n",
    "                    j,target_layer\n",
    "                ),\n",
    "            ),\n",
    "            gradient=torch.mul(regions, gradients)[j],\n",
    "        )\n",
    "gcam.remove_hook()\n",
    "gbp.remove_hook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd4d7e-096f-43c5-9c70-d89307a65aac",
   "metadata": {},
   "source": [
    "## GradCam visualization layer wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af9d95fb-3959-4b5f-8396-e12de28f30e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad-CAM/Guided Backpropagation/Guided Grad-CAM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "#target_layer = target_layers[-1]\n",
    "# =========================================================================\n",
    "print(\"Grad-CAM/Guided Backpropagation/Guided Grad-CAM:\")\n",
    "\n",
    "gcam = GradCAM(model=model,proxy_embeddings=proxy_embedding)\n",
    "_ = gcam.forward(images)\n",
    "\n",
    "gbp = GuidedBackPropagation(model=model,proxy_embeddings=proxy_embedding)\n",
    "_ = gbp.forward(images)\n",
    "\n",
    "\n",
    "# Guided Backpropagation\n",
    "gbp.backward()\n",
    "gradients = gbp.generate()\n",
    "\n",
    "# Grad-CAM\n",
    "gcam.backward()\n",
    "for target_layer in target_layers: \n",
    "    regions = gcam.generate(target_layer=target_layer)\n",
    "    for j in range(len(images)):\n",
    "        # Grad-CAM\n",
    "        save_gradcam(\n",
    "            filename=osp.join(\n",
    "                output_dir,\n",
    "                \"{}-gradcam-{}_180.png\".format(\n",
    "                    j,target_layer\n",
    "                ),\n",
    "            ),\n",
    "            gcam=regions[0, 0],\n",
    "            raw_image=raw_images[0],\n",
    "            paper_cmap=True\n",
    "        )\n",
    "gcam.remove_hook()\n",
    "gbp.remove_hook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
